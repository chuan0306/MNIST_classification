{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###1. Implement the following layers as python functions(both forward and backward propagation)\t\n",
        "* Inner-product layer\n",
        "* Activation layer(Sigmoid or Rectified)\n",
        "* Softmax layer\n",
        "\n",
        "###2. Implement training and testing process\n",
        "* included cross-validation\n",
        "\n",
        "###3. Plot epoch-accuracy curves\n"
      ],
      "metadata": {
        "id": "rlmh1D8ia79e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jgMAQNAnr_k1",
        "outputId": "4d9f9d41-6710-4613-c4a5-d8b3fc150e3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')       "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Lab1 "
      ],
      "metadata": {
        "id": "MUds0dh6WX0G",
        "outputId": "cc36c87e-b187-427a-b929-ad891fe5e6a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Lab1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ukxBJ3dksMZ-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load MNIST dataset \n",
        "image_size = 28           # width and length\n",
        "no_of_different_labels = 10     #  i.e. 0, 1, 2, 3, ..., 9\n",
        "image_pixels = image_size * image_size\n",
        "\n",
        "train_data = np.loadtxt(\"mnist_train.csv\", delimiter=\",\")                   \n",
        "test_data = np.loadtxt(\"mnist_test.csv\", delimiter=\",\")                    \n",
        "\n",
        "# data preprocessing\n",
        "\n",
        "# map pixels information from range(0, 255) to range(0.01, 1)\n",
        "fac = 0.99 / 255\n",
        "train_imgs = np.asfarray(train_data[:, 1:]) * fac + 0.01                  \n",
        "test_imgs = np.asfarray(test_data[:, 1:]) * fac + 0.01                   \n",
        "\n",
        "train_labels = np.asfarray(train_data[:, :1])                         \n",
        "test_labels = np.asfarray(test_data[:, :1])                          \n",
        "\n",
        "train_labels_for_prediction = train_labels[0:50000]\n",
        "validation_labels_for_prediction = train_labels[50000:60000]\n",
        "\n",
        "lr = np.arange(no_of_different_labels)\n",
        "\n",
        "# transform labels into one hot representation\n",
        "train_labels_one_hot = (lr==train_labels).astype(np.float64)                  # 60000*10 \n",
        "test_labels_one_hot = (lr==test_labels).astype(np.float64)                   # 10000*10 "
      ],
      "metadata": {
        "id": "j8seIvepsQ1a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate train_imgs, train_labels into training and validation\n",
        "training_set = np.transpose(train_imgs[0:50000])                      # 784*50000 轉置\n",
        "validation_set = np.transpose(train_imgs[50000:60000])                   # 784*10000 轉置\n",
        "\n",
        "# Testing_set preprocess\n",
        "testing_set = np.transpose(test_imgs)\n",
        "\n",
        "# Separate train_labels_one_hot into training and validation\n",
        "train_labels_one_hot_temp = train_labels_one_hot                         # 作暫存處理_切出training/testing set  \n",
        "one_hot_train_lab_set = np.transpose(train_labels_one_hot_temp[0:50000])             # 10*50000 轉置\n",
        "one_hot_validation_lab_set = np.transpose(train_labels_one_hot_temp[50000:60000])         # 10*10000 轉置"
      ],
      "metadata": {
        "id": "8yExx60TCWuq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#innitial value setting\n",
        "def main():\n",
        "  global softmax_output\n",
        "\n",
        "w1 = np.random.rand(10, 784)-0.5     \n",
        "b1 = np.random.rand(10, 1)-0.5      \n",
        "w2 = np.random.rand(10, 10)-0.5     \n",
        "b2 = np.random.rand(10, 1)-0.5  \n",
        "iteration = []              \n",
        "accuracy = []\n",
        "accuracy_val = []\n",
        "loss_train = []           \n",
        "loss_val = []\n",
        "\n",
        "#layer definition\n",
        "def InnerProduct_ForProp(x,W,b):\n",
        "    y = np.matmul(W,x)+b\n",
        "    return y\n",
        "\n",
        "def InnerProduct_BackProp(dEdy,x,W):\n",
        "    m, n = dEdy.shape\n",
        "    x_t = np.transpose(x)\n",
        "    dEdW = 1/n * np.matmul(dEdy, x_t)\n",
        "    dEdb = 1/n * np.sum(dEdy)\n",
        "    return dEdW,dEdb\n",
        "\n",
        "def Softmax_ForProp(x):\n",
        "    y = np.exp(x) / sum(np.exp(x))       \n",
        "    return y\n",
        "\n",
        "def Softmax_BackProp(softmax_output,one_hot_label_set):  # prediction - actual(correct) label--one_hot\n",
        "    dEdx = softmax_output - one_hot_label_set\n",
        "    return dEdx\n",
        "\n",
        "def Sigmoid_ForProp(x):\n",
        "    y = 1/(1+np.exp(-x))\n",
        "    return y\n",
        "\n",
        "def Sigmoid_BackProp(dEdy,x):\n",
        "    return dEdx\n",
        "\n",
        "def Rectified_ForProp(x):\n",
        "    y=np.maximum(x,0)\n",
        "    return y\n",
        "\n",
        "def Rectified_BackProp(dZ2, Z1):\n",
        "    w_t = np.transpose(w2)\n",
        "    dEdx = np.matmul(w_t, dZ2) * Relu_deriv(Z1)\n",
        "    return dEdx\n",
        "\n",
        "def Relu_deriv(Z1):\n",
        "    return Z1>0\n",
        "\n",
        "def get_predictions(softmax_output):\n",
        "    return np.argmax(softmax_output, 0)\n",
        "\n",
        "def get_accuracy(predictions, label):\n",
        "    print(predictions, label)\n",
        "    accur = np.sum(predictions == label) / label.size\n",
        "    accuracy.append(accur)\n",
        "    return  (accur) \n",
        "\n",
        "def get_accuracy_val(predictions, label):\n",
        "    print(predictions, label)\n",
        "    accur = np.sum(predictions == label) / label.size\n",
        "    accuracy_val.append(accur)\n",
        "    return  (accur) \n",
        "\n",
        "def get_accuracy_test(predictions, label):\n",
        "    print(predictions, label)\n",
        "    accur = np.sum(predictions == label) / label.size\n",
        "    return  (accur) \n",
        "\n",
        "def cross_entrophy_loss(ground_truth_one_hot, model_prediction_probability):                   \n",
        "    m,n= ground_truth_one_hot.shape                    \n",
        "    output = - np.sum(ground_truth_one_hot * np.log(model_prediction_probability)) / n\n",
        "    return output"
      ],
      "metadata": {
        "id": "v164Rtl4vDN9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2000):  \n",
        "\n",
        "    ##↓ training_set  \n",
        "    # Forward-propagation\n",
        "    Z1 = InnerProduct_ForProp(training_set, w1, b1)     # Z1: layer1_linear_combination_output\n",
        "    A1 = Rectified_ForProp(Z1)                 # A1: layer1_activation_function(ReLu)_output\n",
        "    Z2 = InnerProduct_ForProp(A1, w2, b2)           # Z2: layer2_linear_combination_output\n",
        "    softmax_output = Softmax_ForProp(Z2)            \n",
        "    # Bakcward-propagation\n",
        "    dZ2 = Softmax_BackProp(softmax_output, one_hot_train_lab_set)      # dZ2: partial differential softmax layer\n",
        "    dw2, db2 = InnerProduct_BackProp(dZ2, A1, w2)           # dw2: differential of w2; db2: differential of b2\n",
        "    dZ1 = Rectified_BackProp(dZ2, Z1)\n",
        "    dw1, db1 = InnerProduct_BackProp(dZ1, training_set, w1)\n",
        "\n",
        "    ##↓ validation_set\n",
        "    # Forward-propagation\n",
        "    Z1_val = InnerProduct_ForProp(validation_set, w1, b1)     \n",
        "    A1_val = Rectified_ForProp(Z1_val)                 \n",
        "    Z2_val = InnerProduct_ForProp(A1_val, w2, b2)           \n",
        "    softmax_output_val = Softmax_ForProp(Z2_val)            \n",
        "    # Bakcward-propagation\n",
        "    dZ2_val = Softmax_BackProp(softmax_output_val, one_hot_validation_lab_set)      \n",
        "    dw2_val, db2_val = InnerProduct_BackProp(dZ2_val, A1_val, w2)           \n",
        "    dZ1_val = Rectified_BackProp(dZ2_val, Z1_val)\n",
        "    dw1_val, db1_val = InnerProduct_BackProp(dZ1_val, validation_set, w1)   \n",
        "\n",
        "    # Parameters Updating (Gradient descent)\n",
        "    LR = 0.6                           # learning rate\n",
        "    w1 = w1 - LR * dw1\n",
        "    b1 = b1 - LR * db1\n",
        "    w2 = w2 - LR * dw2\n",
        "    b2 = b2 - LR * db2\n",
        "    # Use validation data to evaluate model\n",
        "    if i % 10 == 0:\n",
        "      print(\"Iteration: \", i)\n",
        "      iteration.append(i)\n",
        "      iteration_count = len(iteration)\n",
        "      predictions = get_predictions(softmax_output)             #get_prediction_of_training_set\n",
        "      predictions_val = get_predictions(softmax_output_val)\n",
        "      loss_of_train = cross_entrophy_loss(one_hot_train_lab_set, softmax_output)        #get_loss_of_training_set\n",
        "      loss_train.append(loss_of_train)\n",
        "      loss_of_val = cross_entrophy_loss(one_hot_validation_lab_set, softmax_output_val)   #get_loss_of_validation_set\n",
        "      loss_val.append(loss_of_val)\n",
        "      print(\"Training set accuracy is \", get_accuracy(predictions, train_labels_for_prediction.T,)) \n",
        "      print(\"Validation set accuracy is \", get_accuracy_val(predictions_val, validation_labels_for_prediction.T)) \n",
        "      print(\"Training set loss is \", loss_of_train)\n",
        "      print(\"Validation set loss is \", loss_of_val)   "
      ],
      "metadata": {
        "id": "jMLeRKqdvJ88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a29dac4-34da-4ed1-96ee-e8a3fe7f1805"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0\n",
            "[8 6 8 ... 6 8 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.1118\n",
            "[8 8 8 ... 8 8 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.1194\n",
            "Training set loss is  3.97032767268407\n",
            "Validation set loss is  3.9569424978115517\n",
            "Iteration:  10\n",
            "[3 6 3 ... 3 5 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.29388\n",
            "[3 8 6 ... 8 5 5] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.3027\n",
            "Training set loss is  1.9393460677882257\n",
            "Validation set loss is  1.9192496701281176\n",
            "Iteration:  20\n",
            "[3 6 2 ... 3 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.46662\n",
            "[3 8 6 ... 8 5 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.4951\n",
            "Training set loss is  1.4554413962645971\n",
            "Validation set loss is  1.4164193488769292\n",
            "Iteration:  30\n",
            "[3 5 4 ... 3 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.5598\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.5781\n",
            "Training set loss is  1.2036661994814244\n",
            "Validation set loss is  1.1535711447818928\n",
            "Iteration:  40\n",
            "[3 0 4 ... 3 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.66678\n",
            "[3 8 6 ... 8 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.6986\n",
            "Training set loss is  0.9437408369118869\n",
            "Validation set loss is  0.8839307388482075\n",
            "Iteration:  50\n",
            "[3 0 4 ... 3 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.6824\n",
            "[3 8 6 ... 8 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.7044\n",
            "Training set loss is  0.8777010567719444\n",
            "Validation set loss is  0.8148069292505608\n",
            "Iteration:  60\n",
            "[3 0 4 ... 3 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.73826\n",
            "[3 8 6 ... 8 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.7589\n",
            "Training set loss is  0.7571472591102598\n",
            "Validation set loss is  0.69515003291816\n",
            "Iteration:  70\n",
            "[3 0 4 ... 3 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.75842\n",
            "[3 8 6 ... 8 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.7781\n",
            "Training set loss is  0.6950465154276513\n",
            "Validation set loss is  0.6387864810059052\n",
            "Iteration:  80\n",
            "[3 0 4 ... 3 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.78588\n",
            "[3 8 6 ... 8 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8015\n",
            "Training set loss is  0.634173641716667\n",
            "Validation set loss is  0.5825127070191707\n",
            "Iteration:  90\n",
            "[3 0 4 ... 3 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.80582\n",
            "[3 8 6 ... 8 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8202\n",
            "Training set loss is  0.5884991277589056\n",
            "Validation set loss is  0.5406487514500676\n",
            "Iteration:  100\n",
            "[3 0 4 ... 3 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.81788\n",
            "[3 8 6 ... 8 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8315\n",
            "Training set loss is  0.5641479337074264\n",
            "Validation set loss is  0.5179305036820334\n",
            "Iteration:  110\n",
            "[3 0 4 ... 3 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.82832\n",
            "[3 8 6 ... 8 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8421\n",
            "Training set loss is  0.539321709449722\n",
            "Validation set loss is  0.4926070962565757\n",
            "Iteration:  120\n",
            "[3 0 4 ... 3 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.83182\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8499\n",
            "Training set loss is  0.5271143862262042\n",
            "Validation set loss is  0.47856697266649606\n",
            "Iteration:  130\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.85732\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8702\n",
            "Training set loss is  0.4635383507175631\n",
            "Validation set loss is  0.42213085655169663\n",
            "Iteration:  140\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.82094\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8388\n",
            "Training set loss is  0.5492668749861904\n",
            "Validation set loss is  0.49807512190087844\n",
            "Iteration:  150\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.8628\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8739\n",
            "Training set loss is  0.44718939015646897\n",
            "Validation set loss is  0.4092824388711731\n",
            "Iteration:  160\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.86394\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8768\n",
            "Training set loss is  0.4407030300797984\n",
            "Validation set loss is  0.40386135575762006\n",
            "Iteration:  170\n",
            "[3 0 4 ... 3 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.83772\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8542\n",
            "Training set loss is  0.5029710076458149\n",
            "Validation set loss is  0.4596636262976646\n",
            "Iteration:  180\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.8748\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.885\n",
            "Training set loss is  0.41588724263308524\n",
            "Validation set loss is  0.38281425218815035\n",
            "Iteration:  190\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.87728\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8863\n",
            "Training set loss is  0.40803118886045214\n",
            "Validation set loss is  0.3765648487798756\n",
            "Iteration:  200\n",
            "[3 0 4 ... 3 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.84128\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8595\n",
            "Training set loss is  0.4898185735707393\n",
            "Validation set loss is  0.4504290158760369\n",
            "Iteration:  210\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.88078\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8894\n",
            "Training set loss is  0.39803776432049637\n",
            "Validation set loss is  0.3675073244404032\n",
            "Iteration:  220\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.884\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8918\n",
            "Training set loss is  0.38834626766039154\n",
            "Validation set loss is  0.3598815727419227\n",
            "Iteration:  230\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.88572\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8928\n",
            "Training set loss is  0.3834556182835475\n",
            "Validation set loss is  0.3560927903603666\n",
            "Iteration:  240\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.85678\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.871\n",
            "Training set loss is  0.4481128599462304\n",
            "Validation set loss is  0.41247965553379545\n",
            "Iteration:  250\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.88626\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8942\n",
            "Training set loss is  0.38104673047948556\n",
            "Validation set loss is  0.3537025956792583\n",
            "Iteration:  260\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.88896\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8963\n",
            "Training set loss is  0.3721109196776787\n",
            "Validation set loss is  0.34659071984243117\n",
            "Iteration:  270\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89044\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8976\n",
            "Training set loss is  0.36783005324718365\n",
            "Validation set loss is  0.343679046334498\n",
            "Iteration:  280\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.88726\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8944\n",
            "Training set loss is  0.3754107096106349\n",
            "Validation set loss is  0.35254876168777594\n",
            "Iteration:  290\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.8705\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8803\n",
            "Training set loss is  0.41897432622534\n",
            "Validation set loss is  0.3958545298946393\n",
            "Iteration:  300\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.88766\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8944\n",
            "Training set loss is  0.3729245010305028\n",
            "Validation set loss is  0.3511688768114045\n",
            "Iteration:  310\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89062\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8961\n",
            "Training set loss is  0.36430190349019903\n",
            "Validation set loss is  0.34336072240664356\n",
            "Iteration:  320\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.88968\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8957\n",
            "Training set loss is  0.365432624806632\n",
            "Validation set loss is  0.3453098607847307\n",
            "Iteration:  330\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.8874\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8938\n",
            "Training set loss is  0.37182668289812726\n",
            "Validation set loss is  0.3522020788749826\n",
            "Iteration:  340\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89334\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8982\n",
            "Training set loss is  0.3559356471790243\n",
            "Validation set loss is  0.33664018178287036\n",
            "Iteration:  350\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89466\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8997\n",
            "Training set loss is  0.3509963208152138\n",
            "Validation set loss is  0.33218165486942763\n",
            "Iteration:  360\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89026\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8959\n",
            "Training set loss is  0.3638097525167236\n",
            "Validation set loss is  0.34561636766239473\n",
            "Iteration:  370\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.88568\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8908\n",
            "Training set loss is  0.3763421996045336\n",
            "Validation set loss is  0.3578191061734799\n",
            "Iteration:  380\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89818\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9031\n",
            "Training set loss is  0.34247429166508375\n",
            "Validation set loss is  0.32392845823677924\n",
            "Iteration:  390\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89912\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9038\n",
            "Training set loss is  0.33780897147870115\n",
            "Validation set loss is  0.31987904693409047\n",
            "Iteration:  400\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89898\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9027\n",
            "Training set loss is  0.3390758298687678\n",
            "Validation set loss is  0.3221266203040429\n",
            "Iteration:  410\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.87994\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8852\n",
            "Training set loss is  0.3854127305526851\n",
            "Validation set loss is  0.3681854796588708\n",
            "Iteration:  420\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89908\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9025\n",
            "Training set loss is  0.3389368565512604\n",
            "Validation set loss is  0.3214639024099977\n",
            "Iteration:  430\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90172\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9061\n",
            "Training set loss is  0.329410880552452\n",
            "Validation set loss is  0.31190831541978226\n",
            "Iteration:  440\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90242\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9068\n",
            "Training set loss is  0.3267237301493194\n",
            "Validation set loss is  0.3100399076396183\n",
            "Iteration:  450\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90306\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9071\n",
            "Training set loss is  0.32458776890594704\n",
            "Validation set loss is  0.30855951771193507\n",
            "Iteration:  460\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90358\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9068\n",
            "Training set loss is  0.3233391726875925\n",
            "Validation set loss is  0.3079811939118647\n",
            "Iteration:  470\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89892\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9032\n",
            "Training set loss is  0.336410676622854\n",
            "Validation set loss is  0.32186954104829335\n",
            "Iteration:  480\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.8489\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8604\n",
            "Training set loss is  0.4589665790494954\n",
            "Validation set loss is  0.4392438176630607\n",
            "Iteration:  490\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9038\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.908\n",
            "Training set loss is  0.3212868725211022\n",
            "Validation set loss is  0.3050916099288099\n",
            "Iteration:  500\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9049\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.909\n",
            "Training set loss is  0.3180523591920437\n",
            "Validation set loss is  0.30282064698261874\n",
            "Iteration:  510\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90576\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.909\n",
            "Training set loss is  0.3157672837351464\n",
            "Validation set loss is  0.30136523752791555\n",
            "Iteration:  520\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90632\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9096\n",
            "Training set loss is  0.3138374618486105\n",
            "Validation set loss is  0.30001146726747274\n",
            "Iteration:  530\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90694\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9103\n",
            "Training set loss is  0.3120690540973037\n",
            "Validation set loss is  0.29873494760800284\n",
            "Iteration:  540\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9075\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9106\n",
            "Training set loss is  0.31043992478592164\n",
            "Validation set loss is  0.2975530169476783\n",
            "Iteration:  550\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90782\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9124\n",
            "Training set loss is  0.30986252384598173\n",
            "Validation set loss is  0.2977500021124029\n",
            "Iteration:  560\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.82676\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.832\n",
            "Training set loss is  0.6300860443876517\n",
            "Validation set loss is  0.6047123681521039\n",
            "Iteration:  570\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90756\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9125\n",
            "Training set loss is  0.31047327917136996\n",
            "Validation set loss is  0.29663554797646646\n",
            "Iteration:  580\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90832\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9128\n",
            "Training set loss is  0.3072260814210427\n",
            "Validation set loss is  0.2943095435478765\n",
            "Iteration:  590\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90914\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9134\n",
            "Training set loss is  0.30513077771458885\n",
            "Validation set loss is  0.29303734836820045\n",
            "Iteration:  600\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9098\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9141\n",
            "Training set loss is  0.30340282909860306\n",
            "Validation set loss is  0.2919672006665928\n",
            "Iteration:  610\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91014\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9151\n",
            "Training set loss is  0.30184825027851164\n",
            "Validation set loss is  0.2909063011763292\n",
            "Iteration:  620\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91052\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9154\n",
            "Training set loss is  0.3003826369535255\n",
            "Validation set loss is  0.2898974547621961\n",
            "Iteration:  630\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91084\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9154\n",
            "Training set loss is  0.29899459294097486\n",
            "Validation set loss is  0.2889605834831821\n",
            "Iteration:  640\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91152\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9162\n",
            "Training set loss is  0.29773314511883664\n",
            "Validation set loss is  0.28814210441969523\n",
            "Iteration:  650\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91062\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9148\n",
            "Training set loss is  0.300761163227083\n",
            "Validation set loss is  0.2919948555190186\n",
            "Iteration:  660\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89808\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.902\n",
            "Training set loss is  0.34167585045296756\n",
            "Validation set loss is  0.3322095500106891\n",
            "Iteration:  670\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91102\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9175\n",
            "Training set loss is  0.2980702644837335\n",
            "Validation set loss is  0.2871998162400414\n",
            "Iteration:  680\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91186\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.918\n",
            "Training set loss is  0.29554097014049946\n",
            "Validation set loss is  0.2855475389464282\n",
            "Iteration:  690\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91264\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.918\n",
            "Training set loss is  0.2937801928885996\n",
            "Validation set loss is  0.284481469393523\n",
            "Iteration:  700\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9126\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9182\n",
            "Training set loss is  0.2923292915300476\n",
            "Validation set loss is  0.283516318697955\n",
            "Iteration:  710\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91284\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9185\n",
            "Training set loss is  0.29109534043993995\n",
            "Validation set loss is  0.28264137506333165\n",
            "Iteration:  720\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.913\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9184\n",
            "Training set loss is  0.2906796468689065\n",
            "Validation set loss is  0.2823237288823969\n",
            "Iteration:  730\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90756\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9128\n",
            "Training set loss is  0.30701885769206455\n",
            "Validation set loss is  0.2967592012473113\n",
            "Iteration:  740\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90046\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9066\n",
            "Training set loss is  0.3250487817338489\n",
            "Validation set loss is  0.3136332724757707\n",
            "Iteration:  750\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9129\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9188\n",
            "Training set loss is  0.29040421269016115\n",
            "Validation set loss is  0.2819193770496135\n",
            "Iteration:  760\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91362\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9198\n",
            "Training set loss is  0.287819133309346\n",
            "Validation set loss is  0.2800178227812566\n",
            "Iteration:  770\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91438\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9213\n",
            "Training set loss is  0.28615105099057453\n",
            "Validation set loss is  0.278912344157827\n",
            "Iteration:  780\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.915\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9213\n",
            "Training set loss is  0.2848353438493932\n",
            "Validation set loss is  0.27804152654783504\n",
            "Iteration:  790\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91548\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9211\n",
            "Training set loss is  0.283664161178229\n",
            "Validation set loss is  0.2772642821364449\n",
            "Iteration:  800\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91582\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9214\n",
            "Training set loss is  0.2826073727926794\n",
            "Validation set loss is  0.27652147826770374\n",
            "Iteration:  810\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9161\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9216\n",
            "Training set loss is  0.2816816189807583\n",
            "Validation set loss is  0.2757793257353864\n",
            "Iteration:  820\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91492\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9205\n",
            "Training set loss is  0.2827685068149618\n",
            "Validation set loss is  0.2762836860614073\n",
            "Iteration:  830\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.85832\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8694\n",
            "Training set loss is  0.4560296606127175\n",
            "Validation set loss is  0.4259759370968868\n",
            "Iteration:  840\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91538\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9213\n",
            "Training set loss is  0.28348926715341566\n",
            "Validation set loss is  0.2762911108522803\n",
            "Iteration:  850\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91648\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9223\n",
            "Training set loss is  0.2806128481488653\n",
            "Validation set loss is  0.2743797983044636\n",
            "Iteration:  860\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9171\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9224\n",
            "Training set loss is  0.2789605047041387\n",
            "Validation set loss is  0.2733648472046163\n",
            "Iteration:  870\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91762\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9226\n",
            "Training set loss is  0.27771633442501387\n",
            "Validation set loss is  0.2726354175798778\n",
            "Iteration:  880\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.918\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9228\n",
            "Training set loss is  0.2766616781776819\n",
            "Validation set loss is  0.27197665194103643\n",
            "Iteration:  890\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91824\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9227\n",
            "Training set loss is  0.27571009327878704\n",
            "Validation set loss is  0.27134608121122733\n",
            "Iteration:  900\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91842\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.923\n",
            "Training set loss is  0.2748284380473955\n",
            "Validation set loss is  0.27072576563797873\n",
            "Iteration:  910\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91866\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9232\n",
            "Training set loss is  0.2739952875288006\n",
            "Validation set loss is  0.27013292032560915\n",
            "Iteration:  920\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91896\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9229\n",
            "Training set loss is  0.27320347075423174\n",
            "Validation set loss is  0.26955330317260995\n",
            "Iteration:  930\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91922\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9231\n",
            "Training set loss is  0.2724450746838128\n",
            "Validation set loss is  0.26897162380280865\n",
            "Iteration:  940\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91946\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9226\n",
            "Training set loss is  0.2717461352377447\n",
            "Validation set loss is  0.26836443437907925\n",
            "Iteration:  950\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91962\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9224\n",
            "Training set loss is  0.27159325693354974\n",
            "Validation set loss is  0.2679836038087165\n",
            "Iteration:  960\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91484\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9194\n",
            "Training set loss is  0.28234012745884135\n",
            "Validation set loss is  0.27628894211401567\n",
            "Iteration:  970\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.85444\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.8639\n",
            "Training set loss is  0.45893071645154754\n",
            "Validation set loss is  0.43163827000297333\n",
            "Iteration:  980\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91842\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9228\n",
            "Training set loss is  0.2737771454366487\n",
            "Validation set loss is  0.26962126622373\n",
            "Iteration:  990\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91944\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.923\n",
            "Training set loss is  0.2710528177899601\n",
            "Validation set loss is  0.2676719127648867\n",
            "Iteration:  1000\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92012\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9231\n",
            "Training set loss is  0.2694692448871232\n",
            "Validation set loss is  0.266648294855068\n",
            "Iteration:  1010\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92052\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9234\n",
            "Training set loss is  0.26834988876247806\n",
            "Validation set loss is  0.26594311165280626\n",
            "Iteration:  1020\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92098\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9237\n",
            "Training set loss is  0.2674313462810008\n",
            "Validation set loss is  0.26535976897892827\n",
            "Iteration:  1030\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92126\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9241\n",
            "Training set loss is  0.2666093605316355\n",
            "Validation set loss is  0.2648209691375529\n",
            "Iteration:  1040\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92138\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9239\n",
            "Training set loss is  0.2658602875759833\n",
            "Validation set loss is  0.26430172712223293\n",
            "Iteration:  1050\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9218\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9238\n",
            "Training set loss is  0.26516566332978375\n",
            "Validation set loss is  0.2637941319759263\n",
            "Iteration:  1060\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92204\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9244\n",
            "Training set loss is  0.2645108754501224\n",
            "Validation set loss is  0.2632971119747313\n",
            "Iteration:  1070\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92224\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9246\n",
            "Training set loss is  0.2638779741958192\n",
            "Validation set loss is  0.26281315394769283\n",
            "Iteration:  1080\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92248\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9249\n",
            "Training set loss is  0.26326234908169244\n",
            "Validation set loss is  0.26235361379379285\n",
            "Iteration:  1090\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92268\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9248\n",
            "Training set loss is  0.2626663672470361\n",
            "Validation set loss is  0.26190837021809926\n",
            "Iteration:  1100\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92286\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9245\n",
            "Training set loss is  0.26208561420700915\n",
            "Validation set loss is  0.26147012363775496\n",
            "Iteration:  1110\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92308\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9248\n",
            "Training set loss is  0.2615239009774914\n",
            "Validation set loss is  0.2610167900837785\n",
            "Iteration:  1120\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92328\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9249\n",
            "Training set loss is  0.26099664783304977\n",
            "Validation set loss is  0.2605358529458191\n",
            "Iteration:  1130\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92314\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9251\n",
            "Training set loss is  0.2607999317244047\n",
            "Validation set loss is  0.26015113883487007\n",
            "Iteration:  1140\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.91986\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9233\n",
            "Training set loss is  0.2675240448476309\n",
            "Validation set loss is  0.26510058002679254\n",
            "Iteration:  1150\n",
            "[3 0 4 ... 8 9 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.86114\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.871\n",
            "Training set loss is  0.4551102939359465\n",
            "Validation set loss is  0.43013095312584576\n",
            "Iteration:  1160\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92254\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9244\n",
            "Training set loss is  0.26306354952497313\n",
            "Validation set loss is  0.26168800672261827\n",
            "Iteration:  1170\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92374\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.925\n",
            "Training set loss is  0.2607029612543002\n",
            "Validation set loss is  0.25997719920866313\n",
            "Iteration:  1180\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92414\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9253\n",
            "Training set loss is  0.2594054602602227\n",
            "Validation set loss is  0.2591566497782111\n",
            "Iteration:  1190\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92448\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9253\n",
            "Training set loss is  0.25847440609159666\n",
            "Validation set loss is  0.25854759903374513\n",
            "Iteration:  1200\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92482\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9253\n",
            "Training set loss is  0.25770263121096176\n",
            "Validation set loss is  0.2580148697563454\n",
            "Iteration:  1210\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9251\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9255\n",
            "Training set loss is  0.25702574872353656\n",
            "Validation set loss is  0.25753545293067603\n",
            "Iteration:  1220\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92544\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9257\n",
            "Training set loss is  0.256407954407275\n",
            "Validation set loss is  0.25708250516136333\n",
            "Iteration:  1230\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92566\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9259\n",
            "Training set loss is  0.2558242200619372\n",
            "Validation set loss is  0.25665469635980864\n",
            "Iteration:  1240\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92574\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9258\n",
            "Training set loss is  0.2552733243614729\n",
            "Validation set loss is  0.2562547203290595\n",
            "Iteration:  1250\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92616\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9256\n",
            "Training set loss is  0.25474959235799916\n",
            "Validation set loss is  0.2558803602158672\n",
            "Iteration:  1260\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92626\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9257\n",
            "Training set loss is  0.2542459062027103\n",
            "Validation set loss is  0.25551121378026803\n",
            "Iteration:  1270\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92632\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9259\n",
            "Training set loss is  0.25375616312296456\n",
            "Validation set loss is  0.2551488483347521\n",
            "Iteration:  1280\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92644\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9261\n",
            "Training set loss is  0.25329403794523614\n",
            "Validation set loss is  0.2548210730234214\n",
            "Iteration:  1290\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9268\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9263\n",
            "Training set loss is  0.25292871347169343\n",
            "Validation set loss is  0.2546201377859499\n",
            "Iteration:  1300\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9269\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9273\n",
            "Training set loss is  0.25305320595886793\n",
            "Validation set loss is  0.2549773956466775\n",
            "Iteration:  1310\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9247\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9259\n",
            "Training set loss is  0.25651208015441024\n",
            "Validation set loss is  0.2587358409801289\n",
            "Iteration:  1320\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.90956\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9136\n",
            "Training set loss is  0.2952442595539292\n",
            "Validation set loss is  0.297097633433498\n",
            "Iteration:  1330\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9228\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9253\n",
            "Training set loss is  0.2609699967765523\n",
            "Validation set loss is  0.26253802485260963\n",
            "Iteration:  1340\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92582\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9263\n",
            "Training set loss is  0.2533126520678516\n",
            "Validation set loss is  0.255352016954537\n",
            "Iteration:  1350\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92674\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9266\n",
            "Training set loss is  0.2516186708056097\n",
            "Validation set loss is  0.25388937620647467\n",
            "Iteration:  1360\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92716\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9266\n",
            "Training set loss is  0.2506526528626227\n",
            "Validation set loss is  0.2531073607034596\n",
            "Iteration:  1370\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92744\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9267\n",
            "Training set loss is  0.24994571706157695\n",
            "Validation set loss is  0.2525679305101095\n",
            "Iteration:  1380\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92778\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9271\n",
            "Training set loss is  0.24935449374035287\n",
            "Validation set loss is  0.2521141873074013\n",
            "Iteration:  1390\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92804\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9272\n",
            "Training set loss is  0.24881862411660627\n",
            "Validation set loss is  0.25170190358016226\n",
            "Iteration:  1400\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92806\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9277\n",
            "Training set loss is  0.248321469006241\n",
            "Validation set loss is  0.25132036843002836\n",
            "Iteration:  1410\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92828\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.928\n",
            "Training set loss is  0.24784555133727362\n",
            "Validation set loss is  0.25095567574844946\n",
            "Iteration:  1420\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92856\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9283\n",
            "Training set loss is  0.2473870290055899\n",
            "Validation set loss is  0.25061626821432376\n",
            "Iteration:  1430\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92876\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9282\n",
            "Training set loss is  0.2469401998432649\n",
            "Validation set loss is  0.2502987289141389\n",
            "Iteration:  1440\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92894\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9285\n",
            "Training set loss is  0.246498735731604\n",
            "Validation set loss is  0.24997150442084157\n",
            "Iteration:  1450\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92902\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9287\n",
            "Training set loss is  0.24606327980006065\n",
            "Validation set loss is  0.24966076043646307\n",
            "Iteration:  1460\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92924\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9288\n",
            "Training set loss is  0.24563527544242922\n",
            "Validation set loss is  0.24935290610231908\n",
            "Iteration:  1470\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92942\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9287\n",
            "Training set loss is  0.2452149720117259\n",
            "Validation set loss is  0.2490324827265562\n",
            "Iteration:  1480\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92956\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.929\n",
            "Training set loss is  0.24479842149091222\n",
            "Validation set loss is  0.2487152294705425\n",
            "Iteration:  1490\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9296\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9292\n",
            "Training set loss is  0.2443861417057446\n",
            "Validation set loss is  0.24839956353827702\n",
            "Iteration:  1500\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92976\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9294\n",
            "Training set loss is  0.24398759377603582\n",
            "Validation set loss is  0.2480728567405954\n",
            "Iteration:  1510\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9298\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9291\n",
            "Training set loss is  0.24369846114567306\n",
            "Validation set loss is  0.24774313911455756\n",
            "Iteration:  1520\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92878\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9293\n",
            "Training set loss is  0.24461932681406576\n",
            "Validation set loss is  0.24817831003229968\n",
            "Iteration:  1530\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92052\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9222\n",
            "Training set loss is  0.2633166167950289\n",
            "Validation set loss is  0.26377182020447215\n",
            "Iteration:  1540\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.89776\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9044\n",
            "Training set loss is  0.3265848778537344\n",
            "Validation set loss is  0.3229243987444006\n",
            "Iteration:  1550\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92888\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9278\n",
            "Training set loss is  0.2457967414088561\n",
            "Validation set loss is  0.2501090633391908\n",
            "Iteration:  1560\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93008\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9277\n",
            "Training set loss is  0.24372694637694978\n",
            "Validation set loss is  0.2483469158915797\n",
            "Iteration:  1570\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93026\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9283\n",
            "Training set loss is  0.24265272225360385\n",
            "Validation set loss is  0.24752886946195282\n",
            "Iteration:  1580\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9304\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9285\n",
            "Training set loss is  0.24190871780184475\n",
            "Validation set loss is  0.2469842035864876\n",
            "Iteration:  1590\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93066\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9284\n",
            "Training set loss is  0.2412970000434566\n",
            "Validation set loss is  0.24650134711729324\n",
            "Iteration:  1600\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93102\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9285\n",
            "Training set loss is  0.24075499175021547\n",
            "Validation set loss is  0.246084765288188\n",
            "Iteration:  1610\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93114\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9283\n",
            "Training set loss is  0.24027050147355405\n",
            "Validation set loss is  0.24572141611861034\n",
            "Iteration:  1620\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93132\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9282\n",
            "Training set loss is  0.2398211692140784\n",
            "Validation set loss is  0.24537292725101437\n",
            "Iteration:  1630\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93144\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9282\n",
            "Training set loss is  0.2393933251137632\n",
            "Validation set loss is  0.24503891868855868\n",
            "Iteration:  1640\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9315\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9282\n",
            "Training set loss is  0.2389725669867269\n",
            "Validation set loss is  0.24468611006531082\n",
            "Iteration:  1650\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93154\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9281\n",
            "Training set loss is  0.23856574478637566\n",
            "Validation set loss is  0.24435682520432261\n",
            "Iteration:  1660\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93174\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9282\n",
            "Training set loss is  0.23816823671368564\n",
            "Validation set loss is  0.24403900643155663\n",
            "Iteration:  1670\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.932\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9285\n",
            "Training set loss is  0.23777806422011433\n",
            "Validation set loss is  0.2437248659261158\n",
            "Iteration:  1680\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93222\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9289\n",
            "Training set loss is  0.23739555414807034\n",
            "Validation set loss is  0.24341048075568378\n",
            "Iteration:  1690\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93254\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9288\n",
            "Training set loss is  0.23701543438747455\n",
            "Validation set loss is  0.24309043309014905\n",
            "Iteration:  1700\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93256\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9288\n",
            "Training set loss is  0.23663258985562552\n",
            "Validation set loss is  0.24276521856057906\n",
            "Iteration:  1710\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93266\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9288\n",
            "Training set loss is  0.23624758556474054\n",
            "Validation set loss is  0.24242768542403723\n",
            "Iteration:  1720\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93278\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9291\n",
            "Training set loss is  0.23586503499740363\n",
            "Validation set loss is  0.24209770401103436\n",
            "Iteration:  1730\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93278\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9293\n",
            "Training set loss is  0.23548330203931717\n",
            "Validation set loss is  0.24178643303720027\n",
            "Iteration:  1740\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9329\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9296\n",
            "Training set loss is  0.23510400397647313\n",
            "Validation set loss is  0.24148471456098095\n",
            "Iteration:  1750\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93294\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9295\n",
            "Training set loss is  0.2347302118504596\n",
            "Validation set loss is  0.24119818163051512\n",
            "Iteration:  1760\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93312\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.93\n",
            "Training set loss is  0.23437432393926175\n",
            "Validation set loss is  0.2409545638792541\n",
            "Iteration:  1770\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93332\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9303\n",
            "Training set loss is  0.2340692540384534\n",
            "Validation set loss is  0.24079765407462328\n",
            "Iteration:  1780\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9332\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9301\n",
            "Training set loss is  0.23392535261632158\n",
            "Validation set loss is  0.24087607674949674\n",
            "Iteration:  1790\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93266\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9293\n",
            "Training set loss is  0.23449724105260125\n",
            "Validation set loss is  0.241814511334615\n",
            "Iteration:  1800\n",
            "[5 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93102\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9281\n",
            "Training set loss is  0.2384594777569519\n",
            "Validation set loss is  0.2464496452225113\n",
            "Iteration:  1810\n",
            "[5 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92502\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9242\n",
            "Training set loss is  0.25594529166292446\n",
            "Validation set loss is  0.26481377889411684\n",
            "Iteration:  1820\n",
            "[5 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9224\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9222\n",
            "Training set loss is  0.26312540254884836\n",
            "Validation set loss is  0.27225777016299824\n",
            "Iteration:  1830\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9329\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9296\n",
            "Training set loss is  0.2339187165330636\n",
            "Validation set loss is  0.2418983367898746\n",
            "Iteration:  1840\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93386\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9299\n",
            "Training set loss is  0.23192203099855901\n",
            "Validation set loss is  0.23943930571741162\n",
            "Iteration:  1850\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9338\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9307\n",
            "Training set loss is  0.2313575254227785\n",
            "Validation set loss is  0.23882587289446305\n",
            "Iteration:  1860\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93396\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9309\n",
            "Training set loss is  0.23091801396164427\n",
            "Validation set loss is  0.23841584973598792\n",
            "Iteration:  1870\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93412\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.931\n",
            "Training set loss is  0.23051465110880953\n",
            "Validation set loss is  0.23806521979083642\n",
            "Iteration:  1880\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93406\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9308\n",
            "Training set loss is  0.23013414298688706\n",
            "Validation set loss is  0.2377541856389991\n",
            "Iteration:  1890\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93412\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9309\n",
            "Training set loss is  0.2297640195941697\n",
            "Validation set loss is  0.23746329010434755\n",
            "Iteration:  1900\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93434\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9309\n",
            "Training set loss is  0.22940751856811933\n",
            "Validation set loss is  0.2372003178829377\n",
            "Iteration:  1910\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93452\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9308\n",
            "Training set loss is  0.2290690204971107\n",
            "Validation set loss is  0.23696552223513676\n",
            "Iteration:  1920\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93468\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9306\n",
            "Training set loss is  0.22876824088004483\n",
            "Validation set loss is  0.23680353342808108\n",
            "Iteration:  1930\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93482\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9308\n",
            "Training set loss is  0.22858389337383003\n",
            "Validation set loss is  0.23680778387548243\n",
            "Iteration:  1940\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.9346\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9308\n",
            "Training set loss is  0.22880545482886344\n",
            "Validation set loss is  0.237309972026961\n",
            "Iteration:  1950\n",
            "[5 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93406\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9303\n",
            "Training set loss is  0.230659267874616\n",
            "Validation set loss is  0.2396188724479798\n",
            "Iteration:  1960\n",
            "[5 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93102\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9283\n",
            "Training set loss is  0.23913202929738192\n",
            "Validation set loss is  0.24879036175534003\n",
            "Iteration:  1970\n",
            "[5 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.92184\n",
            "[3 8 6 ... 5 6 0] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9211\n",
            "Training set loss is  0.2635531402574865\n",
            "Validation set loss is  0.27362184104706566\n",
            "Iteration:  1980\n",
            "[5 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93194\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.9293\n",
            "Training set loss is  0.23573038419596765\n",
            "Validation set loss is  0.24583750239362634\n",
            "Iteration:  1990\n",
            "[3 0 4 ... 8 4 8] [[5. 0. 4. ... 8. 4. 8.]]\n",
            "Training set accuracy is  0.93534\n",
            "[3 8 6 ... 5 6 8] [[3. 8. 6. ... 5. 6. 8.]]\n",
            "Validation set accuracy is  0.931\n",
            "Training set loss is  0.2271497646360056\n",
            "Validation set loss is  0.23617367796388172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HPP0zrqJVLn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using test_images and test_labels to do the final test\n",
        "# forward propagation\n",
        "Z1_test = InnerProduct_ForProp(testing_set, w1, b1)     \n",
        "A1_test = Rectified_ForProp(Z1_test)                 \n",
        "Z2_test = InnerProduct_ForProp(A1_test, w2, b2)           \n",
        "softmax_output_test = Softmax_ForProp(Z2_test)\n",
        "\n",
        "# prediction and get accuracy\n",
        "predictions_test = get_predictions(softmax_output_test)  \n",
        "print(\"Testing set accuracy is \", get_accuracy_test(predictions_test, test_labels.T)) "
      ],
      "metadata": {
        "id": "YkIarEYOsNIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3dd72f-6b31-4b18-b5c8-576a260f43ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7 2 1 ... 4 5 6] [[7. 2. 1. ... 4. 5. 6.]]\n",
            "Testing set accuracy is  0.9302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot accuracy and loss curves\n",
        "plt.title(\"Accuracy of Training set\")\n",
        "plt.plot(iteration, accuracy, color=\"red\")\n",
        "plt.plot(iteration, accuracy_val, color=\"blue\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt,plt.xlabel(\"Iteration\")\n",
        "plt.show()  \n",
        "\n",
        "plt.title(\"Loss of traing set and validation set\")\n",
        "plt.plot(iteration, loss_train, color=\"red\", label=\"loss of training set\")\n",
        "plt.plot(iteration, loss_val, color=\"blue\", label=\"loss of validation set\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt,plt.xlabel(\"Iteration\")\n",
        "plt.show()   "
      ],
      "metadata": {
        "id": "83d2qq4oCvy6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "fdba6ae5-28cf-44f3-ce89-58122f9a1270"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gc1dX48e/ZlVa7qi6Sq5ptbMAYDMYUUxICDrGpoYReQichhUDeBJL3JbyQThKSX0II8JLQwRAIOKH3XmzAGBtjMGBcsLHlor6rlfb8/rgjey0kWbI1u7LnfJ5nnp2ZnZ05OyvdM/fOzB1RVYwxxgRXKNsBGGOMyS5LBMYYE3CWCIwxJuAsERhjTMBZIjDGmICzRGCMMQFnicCYLSAi+4vIhyLSICJf93E7j4rImX29rDHpxO4jMFtCRJ4DJgLDVDWR5XAyTkSeBmaq6p86ea8hbTIfSABt3vQFqnpnBkLMOhG5EthBVU/Ldiyme1YjML0mItXAgYACR2V42zmZ3F43qoD5nb2hqoXtA7AEODJt3oYk0I++iwk4SwRmS5wBvAbcAmzSFCEiFSLygIisFpE1IvKXtPfOE5EFIlIvIu+JyCRvvorIDmnL3SIiP/fGDxKRZSLyYxFZCfxDRAaKyH+8bazzxsvTPj9IRP4hIp957z/ozZ8nIkemLZcrIjUiskdnX9KLd5GIrBWRmSIywpv/ETAa+LfXNJTXk522hd/lORE51xv/poi8JCK/85b9RESmb+Gyo0TkBe+3eEpErhORO7qIu9SLa723L14UkZD33ggRud+L/xMR+Z43fxrwE+BEbx+905N9ZLLDEoHZEmcAd3rD10RkKICIhIH/AJ8C1cBI4B7vvW8AV3qfLcbVJNb0cHvDgEG4o/DzcX+3//CmK4Fm4C9py9+Oa5LZBRgCXOvNvw1Ib6Y4DFihqm933KCIHAz8CjgBGO59p3sAVHUMmx7p96ZprLffpaN9gIVAKfBb4GYRkS1Y9i7gDWAw7nc5vZttXgosA8qAobgCXr1k8G/gHdxvfQhwsYh8TVUfA34JzPD20cRu1m+yTVVtsKHHA3AAkARKven3gR9441OA1UBOJ597HPh+F+tUXFty+/QtwM+98YOAFiDaTUy7A+u88eFAChjYyXIjgHqg2Jv+J/CjLtZ5M/DbtOlC73tXe9OLgak92F8bluvtd/GmnwPO9ca/CSxKey/f23fDerMsLuG0Avlp798B3NFFTFcBD6X/Rt78fYAlHeZdDvzDG7+yq3Xa0L8GqxGY3joTeEJVa7zpu9jYPFQBfKqqrZ18rgL4aAu3uVpV4+0TIpIvIjeIyKciUge8AAzwaiQVwFpVXddxJar6GfAycJyIDACm42o1nRmBqwW0f7YBV4MZuYXfYUu+S2dWpsXU5I0W9nLZEbh91JS27NJuYr4GWAQ8ISIfi8hl3vwqYITXZLReRNbjagtDu1mX6YfsZJXpMRGJ4ZpKwl4bN0AeruCaiCtMKkUkp5NksBQY08Wqm3BHrO2G4Zoi2nW8tO1SYEdgH1VdKSK7A28D4m1nkIgMUNX1nWzrVuBc3N/+q6q6vIuYPsMVdACISAGuGaWr5XuqN9/FLytw+yg/LRlUdLWwqtZ7cV4qIhOAZ0RkFm5ff6KqY7v6aF8GbfxjNQLTG1/HXQY5HteEsTuwM/Airu3/DVwh82sRKRCRqIjs7332/4Afisie4uwgIu0F7RzgFBEJeycZv7yZOIpwbenrRWQQ8LP2N1R1BfAo8FfvRGyuiHwp7bMPApOA7+POGXTlbuAsEdndOxn8S+B1VV28mdh6q8vv4hdV/RSYDVwpIhERmQIc2dXyInKE93sJUIv7G0jhfu967+R3zPv9JojIXt5HPweq208sm/7LfiDTG2fi2n+XqOrK9gF3cvNU3FHskcAOuJOpy4ATAVT1PuAXuKakelyBPMhb7/e9z6331vPgZuL4IxADanBXLz3W4f3Tce357wOrgIvb31DVZuB+YBTwQFcbUNWngP/xll2Bq82ctJm4tsTmvotfTsWd01kD/ByYgbvfoTNjgaeABuBV4K+q+qyqtgFH4A4IPsF9h/8DSrzP3ee9rhGRt/z4EqZv2A1lJnBE5ApgnNqNThuIyAzgfVX1vUZi+h+rEZhA8ZpfzgFuzHYs2SQie4nIGBEJec1xR7P5mpjZTlkiMIEhIufhTnA+qqovZDueLBuGu9y0Afh/wLe0k/spTDBY05AxxgSc1QiMMSbgtrn7CEpLS7W6ujrbYRhjzDblzTffrFHVss7e2+YSQXV1NbNnz852GMYYs00RkU+7es+ahowxJuAsERhjTMBZIjDGmICzRGCMMQFnicAYYwLOEoExxgScJQJjjAm4be4+AmOM6Zfa2mDZMmhqglTqi0NPRSIwYQKkP4p6/Xr47W/h3HNh9Og+D90SgTHGX6qQTEJLC7S0kIq30NLQQktjkpamVqI5reQV55EcWU1+cYciqaUFFiyAmpoNnyeR6Hy8u/c2M96WaKUpHqIxkUM8GSYcUuISYz0DWK8ltEouBeE4oyPLGFkuyCEHwyWXwCD3SA39819YdOn11CWjLKOczxlKCxES5NFKDiFShEgh6CYDsMl0Pk0M5XMO2GU9Jb/7H5g2Db3/AeaedS1P1u/D9PAb7HK1JQJjAkHVlVE5OW7oqLUV1q2DeNwdiLa2Qmuijda6JlprG2lrjJNKJEklkkhLglBrC6HWFlqbk8Qb24g3pYg3pWhogI8+L6Q1Wsi+p47ha98oJhSCujp4d1acpU9/QGJ5DS21zSQakyTaP9usxONKQyKX2pZ8apP51LbmU9+WTzKVQ6uGaNUwSc0hQR4tRLyhmLZuip1v7T+X617cDRH44NZX+cnZK0iloBmvUGYASXI3FKwhUoRp23RaUoQEJARJiRAnSjMx6lKFNKbyEVEEEFFCoiRSuSRSkR7/NoWfNTL+tflcfOuvOOn1S0g1NHH8xRU8mJq/Bb9053Lnt/DNw2/n+kee4rSTUtzT+iIAeUNglz7bykbbXO+jkydPVutiwrRrbITPPnOFYcchPx/GjoXcXLfs2rXw9tuwdKk7QE0mvQJ0M0NbGxQWQnGxOwA86igY6j2e/cMP4Y47oLnZFdyJhCucEwlINLeRaGwl3phy43H13hMSSSHeEiKRDJFIhglJivzcJMm2EPHWHBJtLujCvBYOmSr88S+5lJfDwbuu5p2PCqlLxvp0P4ZoI0WYq8/4kLN/NZbdxjWzprHrbQgpoqEWinKaKc5ppiTSRElenMJIkkhuipywS2CRXHVDBPIiSiQPIhEhkueG3IgQb82hpTHJB6+t5bYVX+WK0z/mf28bzX9NfpZr3zyQnSsaiBWEGVCcoqQ4RV5USBEmJSHaNJyWAkK0qZBKyYbWmEgEolE3FBdDQYGLP5VyyVbVLVNQ4Ib8fLdsW5t7HTDADTk5UF8PixbB++/Ds/9p4N2PC9k3ZxZDo3U81HAIl3+nnn2mFjFyJAwf7j4fiUA47LbTHlP7dmHjePvQ1ASLF8Pdf2/ihtvy2YO3eJtJ/Pj8dXz3ioGMHLnlv7GIvKmqkzt9zxKBybaPP4aFC10zaHoB3F5QFxfDqaduLNCffx7+8Q+YNw/mzHH/tF2JROC+++CII2DYMFi9umcx5YZayQl5R5ekaGzNo03DAEQjbfz8l2EuvRQuOKWeG+8uIhpuIY8EeZogSjN5qTh5xIkSd/NJdDmeR4IUIRopJJKTIpaTJJrTSlSbWdxYyg1cyDWXruT4M/IZNbGYaTlPMmXYYgYWtRIrCJGTH3FDQR45BXmEC6LkxHIJ5eUikVzIzSWVEyEVdvNiRTlEC3OIFuWSX5JL1Q656MIPOPnQGp5P7MuhBzTz0PMl3L3rr9jp2wcT220skWGDiBRGiMVcAZeTs2kTdl/Q+gbOqniSW2uP4cN34xy/71IGhet4pnbPvt1QH2hrgxv/ZynXX9fGu3XVXPKVt/n9M3v06TYuPXoRf5i5A8dNWMh9c3fc6v1ticD4LpGANWvcUXdNDaxYAcuXu+Gzz6C21v3zLF/uCvd99oGrr3aFfGWlO7LvzpFHwowZbjujRikhbWOPUbXsW76MHfMWk1O3hvD6NYTrawnHGwnHG6kNDeS8z6/m22c08sNfDGTECPjRgBu5sPY35GkzObR2OoRQCIUgFnNDfj4ajdEcHcgnK2NcsOoqFuTvyZrGGIdVzWPVkjizK451J/HKy6GkxH2xoiL32j5eVOQOO9tL046vublfLF1feYWS/XfhjCmL+MYFg/jyN0fxxE+e46u/OKjPf8N3/+s2Jv7uNJQQF8du4No1Z7jYMmjpLU9TddZXuPDwpVz/cBW/mjKTy145KqMx9NbqJc2UVsT6PDGmUvDoHTV85bjB5Bds/cq7SwR2jsB0KpmETz5xTR4tLe5IetUq9/r55/DKK64KO3euK/QnT3bLdVQQaWFkYS0Dwg2E2lrYOfw5qvDAPfuwdnkbhxweo7FReGDHy9l57cvkrl5ODq2EaSOXJLkkuZtT+O5//sz06TBxN1i/XpjDnkycMxfmeBsaNAhGjIDhZa7ALSyEptX88sFFLH4VFi+IAjG+VPoeoy462bXtlJZuHIqLXbtAe+EfiWxSKAuQD+zS0MDhu93Fy58cQGMjLFtXyKji1bBkiT8/xH77UR2ax+IVEZa81wBA5fhCXza160+O5NRr7+Hhtmn89IeJjCcBgIoTpvDls1/kbw8fCMBXp4UzHkNvlVX6s59CITj8jFJf1t2RJYIAUYUnn4Rnn3VH7TU1rj27pATOOAOmT3dl3113wTe/qSSTnR+F5EmCETmrWJGs4MO36vl0YZyWljKuilzNTi1zGcRahrOCkSynuKUOqc91BXXZYPeqyq9e/wo/efZq3nirhf15g2OKn4b9JkD11I2NsuEwRCJc9PDDDLr/FM548Q6efz7MSdzNxB9PhyP/6hX+XoNsJ6qjT7N49Rg+nbMOiFF1zlS47Igt34mFhVTuUQqfwNKPWljWNJADR7675evrgaqC1SxeO4pPP1wDQOUeg/3Z0MCB3HTO69Te/wtKf/iqP9vYnPx8Ttv5TZ5778sMpoY9juv7K2TMF1ki2M4sXAgXXOBOauXnw377wZ//7Ar4bxyvPPa4kBtuozSvntKc9eS1NTEnXsaMGWWceaZyyy3CUzd+TGFyANfyA4qpI5ckpaxhyMAkZWVQOLSAN2t3YK+5N1Mz9zNqFjQBZVx47GrKJu3tCuf2Ydgwd7Tdod78nauu5Xc/W0NN7WD+NGgGvPSSOwrvzJlncvIllzDgT0fw8/DP+PnwG+DqJzeeNOhGdcl6Xl83iMXvfQ5A1cQBW7uLqRjj4vzgjfWsaxtCeVknVaE+VDWwjueXl7Jk6VrKWEVs9HDfthX98zVEf93ofrMsOe6UPC767zhfzXmW0E7HZi2OILFE0E80NcGsWfDBB+7IvLx80/cbGjY2W4vAa6/Btde61332gd/9zrW1X/rdFt5+tY1jBz5L7aoot956MJN3rCcchsceL+LX/JgftF1LJFrkjqSHD6elMcm5r57N7bedxg03CB9/kGR8eCFn3nQQ7Lgj7LADDB7sjtA9pQ/OgWNg9dI4NZ+3IaQYdM3lUN6zQqro0vP532uu5m8Np3HczyZ0nQTAffE//IHpa85k+h1T4Op/9CgJAFQPaWLtqmLmzV/NINZQtOOIHn2uO+1NM68+lwBg5Ah/z7NVDU1Qt6SQuUsHUBVeDtEh/m0sEun+t8iAAcd8hWf++2CqJo+A8DeyGktgqOo2Ney55566LUsmVRcsUH35ZdVUSrWxUfX001Xz8zdeRBaJqF59tVv+uedUJ09WFXHvDRyoOm2aaiiU0iEF9fr1Ea9pLLdF8/NT+tOfumV+GfqJ6gEHqE6frhN5W/euXKH7Vi3XCczV1M1/dxtNl0rpHV++UUF1/jtJrYh+rqcP+k+336Nh3icKqr85YZZ+e+9ZOpjVqolE73bGzTerTpmiGo/3bPmWFtUXX3Q7rodmHHaLgmp58XqdxOyeb6u7MGa/o0KbfnmHpQqqT3/vwa1eZ3fuPf4eBdUcSeqxxU/6uq1+IZVyfxe//322I9muALO1i3LVagQZEo/D5ZfD7be7q2sAnnnGXVFz++1w1pE1HFv4BBWr3uSKtRdzxRXlnHCCcOGFSkNNnP8Z+xAFeUkW5OzGc3PGcVbBTP5Qfz7FJYV8mszhlPxH+MUvdmEIn/O98+LwN3cDyukj/8APl1wCwDUj70XOvviLwYkw7oAh8Dy8+8JalsVLGVNd3+33ya8YTJRmalYpNetyKA2thUgvT2ydfbYbeio3Fw44oFebqB7rag7L6krYK28l5G39pYi51SMZxkpmLXaPfy0fl7/V6+xO9Th3hN6qOVQNrPN1W/2CiLsawWSMJQKfPPEEzJzpLou84Qa49Vb44x/h+CMTHD5sNhf+fW9mPiCsqUkxOKeJm/49lLAoDB3KDSvv5DFZwvHHR3j/feFOzuGUyPOgBa7xX9WddH39SdhrL6puvJFnL96PX/N9JufOpeBn12+I45TTw/zoN+5C+1PP6PoKjHF7u7bzJx5pRQkxurK12+8nRYWUsoyaNUJNXS6luf2zgKqesPEKm+qSdX2z0kGDqJQ3eL3VNTON3GXrzzt0p2rCxvb6ymEJX7dlgskSgQ+SSTjmmI13r559Nrz+aoqyWBP3PjkEiTczg0d4+P4DqW8O89XWRwn//Cr49rehpIRhV1/N6Vfexs3vnkt5aDnfOLQBZi52R8TLl8ODD8LUqa79HuCCC4gcdBBXXHQRHHqoa/v3DD/nME78zYwN410pmVDBUFby2Muu0Bk9djOX7YlQlrue1etzWd2Yz+jYmq3aZ34pG19GjCaayadqSLxvVipCRf4aXm+EAayjYPTQvllvF8p2GbLxO1RsW/f9mG2DdUPdR5qa4J573DX18+e76T+e4y4rfPEF5fWn6ti7+TnkuGPh1Vc5PPQYH64oZOX6GIfmPgc//jEMHOhOjH73u1ya+2dySPLD1G/J/da5G0+OjhwJF120MQm023FHeOop+NGPNp0/dix3Tb6Wu/b6I4wZ0/UXqKhgHB/wWZ07gh4zsWCz37k0r4GahjxqEkWUFvVRIdvHpLKCKj4FoKqy7wrRigGu6Wwky92VUT6S8pFU4u5TqBqb3RO5ZvtkiaAPvPQSVFXBySfDz37mrv4B+Nrfvs4uzOPhfzaxYHkx+4TfhFtugX335bAJG29AOnT3VZv2LDZoEDsfuzOfMIrvld7tLiPaGv/+t2un6k5uLuMKPwMgRhNDd938UW5pQROrmwupSZZQVpLcuhj9Mnw41V4iqN6h7yrAlUNcE0157ir/r7IZOJCq0FK33V2K/N2WCSRLBFvojjvcTVgA//d/rvuEKVPgscdg1ksJBrKW0YftzAG8xCtzClBC7F2xYkOBP/orVewsC9hF5jPyS50cqZ91FuUsR049pceXSnZp2LAeHbWOK1vvYuNjpLJis8uXFrWwJD6EJBFKB/fTJotwmOoC18FQ1YS+uyO3otx93/LC9X22zi6JMKZoFUXUMXjHzNxpaoLFEsEW+v3v3dU+a9fC3LnKXuMbOGuPOSxdCvc/GGIys5HzzuXA0MarH/aamHbj0b77MkNP4G49Cfbd94sb+OpX4U9/cpcaZci4ahffaFnco8RRNqiVJO5ouLSsjzta6UPHV73B2dzMgHF915ZfOcYl55GDmvpsnd356S4P8SjTkfKt6H7SmC5YItgCixe7Xi/B3dA1f24bE1/+K1/7q+sca21dLnsxCyZN4sAdVgAwlg8YtHvlxpVMmcKuzGNX5rk7wjoKheB739vY33EGjNvZnSAeU7TKbX8zSks3Fv6lw7ey1uKjQyas4mbO7VEtp6d22DVGIfXsXtlHVyJtxsgxUfYPvw5lZRnZngkWu2poCzz44Mbxu+6ClrYcdhu4jMpxIxj/zke8Fx/DXtF5UFFB5Z5l7PTBAr7M8zB+/MYPVla6o26RL95GnCU77FnCHrzFwRUf9mj50qEbrywqq+i8r59+obra3RU9YuvvKm43cMchrGQY+ft2cl+GH0480SWBcP/vhM1seywRbIEHH4QJBZ8Qb2zl/vt3AITdxrfCoYcx7fWHeI9L2Gt8oyvkd9uNV++eQpQ47Dxr40pE3BF/+3g/EBlbxVvsCXuc1qPly8rzNoyXVm3+KqOs+f734aCDIC9vs4v2WFUVBTRBVeXml+0Lhx/uBmN8YImgl+bNgxdfVC4P3csiqlgUH0suLew0KR++9CUu53gO5AVG7um1se+6KwOodU0t48ZturIMtv/3SHW1e63sWeFWWrGx+93SMSU+BNRH2jvA60vl5a4r1/3269v1GpMFdo6gF9as8R5TODDJRa1/ZDLuATk7s4DI+B1gn30oza3j6zwEEya4D+26q3sdM6Zvj0j9MHKkO3o+4YQeLV46yl3KmEsLRVWD/Iysf5o61XXxasw2zhJBL5x/vusb6F/H38lwVjI55x0AJvKOu6ErFoO993YLtyeCigrX4f/OO2cp6l4IhVw/GBMn9mjxwWNc1wplrEYGDfQzMmOMjywR9NBTT8EDD7gbxvb56C7YbTcmTWihmFoO4KWNd/oedJBr829PBCLuJrIrrshW6L7JHTaYAayjNLy+R1cZGWP6J3tmcQ8kk7D77q4H0fkPLCQ6ZQ847zyoq6P2lgcoKlBC9bWu0K+tdbcWT52a0RizZWxoEZWxGp5u7OReCGNMv9HdM4vtMK4Hrr8e3nsP/nDwf4juuYu7O/j002HXXSmhjtBO4zZe+VNSEpgkAHBByQxOL38m22EYY7aCXTW0GTU1rjlo6n5NHHXb8a53z1tugSFDYJ13M1HHDuAC5IdT57jzIMaYbZYlgs24+mqor1f+JBcj0Ty4+WaXBAB22829bgsngv1y333ZjsAYs5UsEXSjpcX1J3TiAZ8x/vmbXN8/aX39M3QoPPJI511EGGPMNsISQTeeesq1/py0+NfuAe7f+tYXF9raLqKNMSbLfD1ZLCLTRGShiCwSkcs6eb9SRJ4VkbdFZK6IdP0IrSyYMQMG5Cc49NMb4aqrtr47aGOM6Yd8SwQiEgauA6YD44GTRWR8h8X+G7hXVfcATgL+6lc8vRWPuz6Fjsl/grxdxrpOv4wxZjvkZ41gb2CRqn6sqi3APcDRHZZRoP3J3CXAZz7G0yvPPgt1dfCNmr/CqafaDVPGmO2Wn6XbSGBp2vQyb166K4HTRGQZ8Ajw3c5WJCLni8hsEZm9evVqP2L9gieegGhuKwfxHBx7bEa2aYwx2ZDtw9yTgVtUtRw4DLhdRL4Qk6reqKqTVXVyWYYezPHEE3Bg4Rxi40cH+j4BY8z2z89EsBxIv9Oo3JuX7hzgXgBVfRWIAll/KOvy5e5O4kPXz7DagDFmu+dnIpgFjBWRUSISwZ0MntlhmSXAIQAisjMuEWSm7acbTz7pXg/Vx+GII7IbjDHG+My3RKCqrcB3gMeBBbirg+aLyFUicpS32KXAeSLyDnA38E3tB73gPfEEDM2vY9foIpg0KdvhGGOMr3y9oUxVH8GdBE6fd0Xa+HvA/n7G0FuJBDz8MBwbew7ZdR+7d8AYs92zO4s7ePJJ77LR0E1wwAHZDscYY3yX7auG+p377oMBhUmmph6H/ftVZcUYY3xhiSBNIgEPPQRH7zCfiLTClCnZDskYY3xniSDNK6+4B4wdl/qne+h8SUm2QzLGGN9ZIkjz0UfudeL8u2DatOwGY4wxGWKJIM2SJRCSFCPalsAxx2Q7HGOMyQi7aijNkiUwMrqGnAFDYO+9sx2OMcZkhNUI0ixZ3EZl4kP4+tett1FjTGBYaZdmyYcJKlOL4aijNrusMcZsLywReFIpWLoqj0qWwPiOz88xxpjtlyUCz6pV0NIaplKWwogR2Q7HGGMyxhKBZ8kS91o5qBFy7By6MSY4LBF4NiSCkW3ZDcQYYzLMEoFnQyIYbbUBY0ywWKnnWfKpUkQ9JaMHZzsUY4zJKKsReJYsaqGSJUhlxeYXNsaY7YglAuDjj+HV10NUsxgqLBEYY4Il8Ilg2TL32IFEQvkZ/wuVldkOyRhjMirwieCZZ2DlSvj3eTPZi9lWIzDGBE7gE0FdnXsdm5gPkQiUlWU3IGOMybDAJ4L6evdavPojKC+3zuaMMYET+FKvrs7dSJy3dJE1CxljAinwiaC+HooL25A3XrdnFBtjAinwiaCuDoq0DlTh/POzHY4xxmRc4BNBfW2K4sYV7hnFo0ZlOxxjjMm4wCeCuiXrKGpdBxdckO1QjDEmKwKfCOrXt1FMHUyalO1QjDEmKwKfCOoacyiiHkpKsh2KMcZkReATQX08h2LqobAw26EYY0xWBD4R1MUjFEfidiOZMSawAl36pVLQkIxSFGvNdijGGJM1gU4EDQ3utTjfEoExJrgCnQja+xkqKkhlNxBjjMmiQCeC9p5Hi4s0u4EYY0wWBToRbKgRlAR6NxhjAi7QJeCGGsHAcHYDMcaYLAp2Iqh1TUJFg3KzHIkxxmRPoBNBfU0CgOLSSJYjMcaY7Al0IqhbFQegeEheliMxxpjs8TURiMg0EVkoIotE5LIuljlBRN4Tkfkicpef8XRUv6YFgKIh+ZncrDHG9Cs5m1tARI4EHlbVXl1sLyJh4Drgq8AyYJaIzFTV99KWGQtcDuyvqutEZEivot9KdWuSREiQV1qUyc0aY0y/0pMawYnAhyLyWxHZqRfr3htYpKofq2oLcA9wdIdlzgOuU9V1AKq6qhfr32r161pdz6MDBmRys8YY069sNhGo6mnAHsBHwC0i8qqInC8imzuMHgksTZte5s1LNw4YJyIvi8hrIjKtsxV525stIrNXr169uZB7rK5W3bMIrAtqY0yA9egcgarWAf/EHdUPB44B3hKR727l9nOAscBBwMnATSLyhcNzVb1RVSer6uSysrKt3ORG9fVqzyIwxgTeZhOBiBwlIv8CngNygb1VdTowEbi0m48uByrSpsu9eemWATNVNamqnwAf4BJDRtQ1hKxGYIwJvJ7UCI4DrgrtRYwAABKtSURBVFXVXVX1mvZ2fFVtAs7p5nOzgLEiMkpEIsBJwMwOyzyIqw0gIqW4pqKPe/cVtlx9Y5giaYB8u2rIGBNcPUkEVwJvtE+ISExEqgFU9emuPqSqrcB3gMeBBcC9qjpfRK4SkaO8xR4H1ojIe8CzwH+p6pot+B5bpC6eS3FuHEQytUljjOl3Nnv5KHAfsF/adJs3b6/NfVBVHwEe6TDvirRxBS7xhoxb1xxlQF5zNjZtjDH9Rk9qBDne5Z8AeOPbfJ8MqRSsbSmkNL8p26EYY0xW9SQRrE5rykFEjgZq/AspM2proU3DlBYlsh2KMcZkVU+ahi4E7hSRvwCCuzfgDF+jyoAaL5UNLk5mNxBjjMmyzSYCVf0I2FdECr3pBt+jyoA13inp0oFt2Q3EGGOyrCc1AkTkcGAXICreFTaqepWPcfmuvUZQOiTQHbAaY0yPbij7G66/oe/imoa+AVT5HJfvaj5vBWDw0B7lQmOM2W715HB4P1U9A1inqv8LTMHd+LVNW7PMXTZaOtKeRWCMCbaeJIK499okIiOAJK6/oW1azfIWckhSNLww26EYY0xW9aRd5N9eR3DXAG8BCtzka1QZULOylVJqkMGDsh2KMcZkVbeJQERCwNOquh64X0T+A0RVtTYj0floTY1SSg0MHJjtUIwxJqu6bRrynkp2Xdp0YntIAgA1a4XBrIFBViMwxgRbT84RPC0ix4lsXz2z1azPtRqBMcbQs0RwAa6TuYSI1IlIvYjU+RyX79Y0RFwisMdUGmMCrid3Fm93T3ZPpWBNU4zBkQbIsfsIjDHBttlSUES+1Nl8VX2h78PJjA0dzhVYF9TGGNOTw+H/ShuPAnsDbwIH+xJRBmzoZ6i4pfsFjTEmAHrSNHRk+rSIVAB/9C2iDNjQ8+gA63DOGGO2pMe1ZcDOfR1IJm3ocG6wZjcQY4zpB3pyjuDPuLuJwSWO3XF3GG+z2puGBlvPo8YY06NzBLPTxluBu1X1ZZ/iyYj6OgWE4rJotkMxxpis60ki+CcQV9U2ABEJi0i+qm6zD/ttrksCEWJl1uGcMcb06M5iIJY2HQOe8ieczGhe6y4bjZZtd7dIGGNMr/UkEUTTH0/pjef7F5L/4rUJckiSU2bdSxhjTE8SQaOITGqfEJE9gW36Tqzm2gQxmq2fIWOMoWfnCC4G7hORz3CPqhyGe3TlNqu5LkmUuPU8aowx9OyGslkishOwozdroaom/Q3LX/GGVqsRGGOMpycPr78IKFDVeao6DygUkW/7H5p/mptxiaDQrhoyxpienCM4z3tCGQCqug44z7+Q/NccF9c0FIttfmFjjNnO9SQRhNMfSiMiYSDiX0j+iyfE1QiidkOZMcb05GTxY8AMEbnBm74AeNS/kPzXnAgTkwSErIsJY4zpSSL4MXA+cKE3PRd35dA2q7klTFF4mz7fbYwxfWazh8TeA+xfBxbjnkVwMLDA37D8FU+GieXYswiMMQa6qRGIyDjgZG+oAWYAqOpXMhOaf5pbc4jltGY7DGOM6Re6axp6H3gROEJVFwGIyA8yEpXPmltzicWsacgYY6D7pqFjgRXAsyJyk4gcgruzeJsXb80lmpvKdhjGGNMvdJkIVPVBVT0J2Al4FtfVxBARuV5EDs1UgH5oTkWIRewxlcYYAz07Wdyoqnd5zy4uB97GXUm0TVKFeCqPWJ7VCIwxBnr5zGJVXaeqN6rqIX4F5LdEwr1G8+x5xcYYA1v28PptWrPXgXYsaonAGGPA50QgItNEZKGILBKRy7pZ7jgRURGZ7Gc8kJYIYpYIjDEGfEwEXp9E1wHTgfHAySIyvpPlioDv425a81087l6j0e3iAihjjNlqftYI9gYWqerHqtoC3AMc3clyVwO/AeI+xrLBhhpBQeBaxYwxplN+loYjgaVp08u8eRt4j8CsUNWHu1uRiJwvIrNFZPbq1au3KqjmRne1UCzfagTGGANZPFksIiHgD8Clm1vWu1JpsqpOLisr26rtNte6PoaiBeGtWo8xxmwv/EwEy4GKtOlyb167ImAC8JyILAb2BWb6fcI4XuuuH7WmIWOMcfwsDWcBY0VklIhEgJOAme1vqmqtqpaqarWqVgOvAUep6mwfY9pQI4gV9aQHbmOM2f75lghUtRX4DvA4rtvqe1V1vohcJSJH+bXdzWmud53NxYotERhjDPTswTRbTFUfAR7pMO+KLpY9yM9Y2sXrXffT0cLcTGzOGGP6vcA1lDd7iSBWsk0/dtkYY/pM8BJBg+t11BKBMcY4gUsE8UaXCKLFlgiMMQYCmAiaG1MIKfIGxLIdijHG9AvBSwRNSpQ4km+JwBhjIICJIN7sEgExSwTGGAMBTATNzRCj2RKBMcZ4gpcI4mKJwBhj0gQuEcQTuKahaDTboRhjTL8QuETQnAgRkwSIdUNtjDEQxETQEiYWTmQ7DGOM6TcClwjiLSGi4WS2wzDGmH4jcImgOZlDLMcSgTHGtAtgIsglltOa7TCMMabfCFwiiLflEMu1RGCMMe0Clwia2yJEI23ZDsMYY/qNQCaCWCSV7TCMMabfCFwiiKcixPIsERhjTLtAJYLWVmgll2ieZjsUY4zpNwKVCJqb3WssZonAGGPaBSoRxOPuNWbdDBljzAaBSgTtNYJozPoZMsaYdsFKBO0Prs+3RGCMMe0ClQjita6zuVhBoL62McZ0K1AlYvN6lwiiBeEsR2KMMf1HsBJBbQsAsUJLBMYY0y5QiSBe73odjRXlZDkSY4zpPwKVCDbUCCwRGGPMBsFKBPWu19FoUW6WIzHGmP4jUIkg3ugSQWxAXpYjMcaY/iNQiaC5wXU2FyuJZDkSY4zpP4KVCBrdDWXREqsRGGNMu4AlAq9GYE1DxhizQaASQbxJyaWFcGEs26EYY0y/EahE0NwMUeKQn5/tUIwxpt8IXCKI0QwxqxEYY0y7QCWCeMJLBBG7asgYY9oFKhE0x0NEJQFi3VAbY0y7YCWCRIhYKJHtMIwxpl/xNRGIyDQRWSgii0Tksk7ev0RE3hORuSLytIhU+RlPPBkiFm7xcxPGGLPN8S0RiEgYuA6YDowHThaR8R0WexuYrKq7Af8EfutXPADNLTlEw61+bsIYY7Y5ftYI9gYWqerHqtoC3AMcnb6Aqj6rqk3e5GtAuY/x0JzMIZaT9HMTxhizzfEzEYwElqZNL/PmdeUc4NHO3hCR80VktojMXr169RYHFG8NE8u1RGCMMen6xcliETkNmAxc09n7qnqjqk5W1cllZWVbvJ3mtgix3LYt/rwxxmyP/HxCy3KgIm263Ju3CRGZCvwU+LKq+npJT3NbhGgk5ecmjDFmm+NnjWAWMFZERolIBDgJmJm+gIjsAdwAHKWqq3yMBYB4W4RYntUIjDEmnW+JQFVbge8AjwMLgHtVdb6IXCUiR3mLXQMUAveJyBwRmdnF6vpEs+YRy7MagTHGpPP14b2q+gjwSId5V6SNT/Vz+5tuF+IaJWo9UBtjzCb6xcniTEh4Zx9iUc1uIMYY088EJhE0N7vXWL71M2SMMemCkwjq3R3F0ZglAmOMSReYRBBfHwesRmCMMR0FJhE0r3cnCWIFgfnKxhjTI4EpFdsTQbQgnOVIjDGmfwlMIojXue6nY0W+XjFrjDHbnMAkguY619mcJQJjjNlUcBKBd9WQJQJjjNlUYBJBvMHVCKLF9uB6Y4xJF5hE0FzvOpuLFedmORJjjOlfgpMIGrxEMMA6GzLGmHSBSQTxJtfraHRANMuRGGNM/xKYRFAUamQcC61GYIwxHQQmEZwz6W0WshN5A2LZDsUYY/qVwCQCxoyB446D/PxsR2KMMf1KcC6qP/poNxhjjNlEcGoExhhjOmWJwBhjAs4SgTHGBJwlAmOMCThLBMYYE3CWCIwxJuAsERhjTMBZIjDGmIATVc12DL0iIquBT7fw46VATR+G05f6a2wWV+9YXL3XX2Pb3uKqUtWyzt7Y5hLB1hCR2ao6OdtxdKa/xmZx9Y7F1Xv9NbYgxWVNQ8YYE3CWCIwxJuCClghuzHYA3eivsVlcvWNx9V5/jS0wcQXqHIExxpgvClqNwBhjTAeWCIwxJuACkwhEZJqILBSRRSJyWYa3XSEiz4rIeyIyX0S+782/UkSWi8gcbzgs7TOXe7EuFJGv+RjbYhF519v+bG/eIBF5UkQ+9F4HevNFRP6fF9dcEZnkU0w7pu2TOSJSJyIXZ2t/icjfRWSViMxLm9frfSQiZ3rLfygiZ/oU1zUi8r637X+JyABvfrWINKftu7+lfWZP729gkRe7+BBXr3+7vv6f7SKuGWkxLRaROd78TO6vrsqHzP2Nqep2PwBh4CNgNBAB3gHGZ3D7w4FJ3ngR8AEwHrgS+GEny4/3YswDRnmxh32KbTFQ2mHeb4HLvPHLgN9444cBjwIC7Au8nqHfbiVQla39BXwJmATM29J9BAwCPvZeB3rjA32I61Agxxv/TVpc1enLdVjPG16s4sU+3Ye4evXb+fE/21lcHd7/PXBFFvZXV+VDxv7GglIj2BtYpKofq2oLcA+QsedWquoKVX3LG68HFgAju/nI0cA9qppQ1U+ARbjvkClHA7d647cCX0+bf5s6rwEDRGS4z7EcAnykqt3dTe7r/lLVF4C1nWyzN/voa8CTqrpWVdcBTwLT+jouVX1CVVu9ydeA8u7W4cVWrKqvqStNbkv7Ln0WVze6+u36/H+2u7i8o/oTgLu7W4dP+6ur8iFjf2NBSQQjgaVp08voviD2jYhUA3sAr3uzvuNV7/7eXvUjs/Eq8ISIvCki53vzhqrqCm98JTA0C3G1O4lN/zmzvb/a9XYfZSPGs3FHju1GicjbIvK8iBzozRvpxZKJuHrz22V6fx0IfK6qH6bNy/j+6lA+ZOxvLCiJoF8QkULgfuBiVa0DrgfGALsDK3BV00w7QFUnAdOBi0TkS+lvekc9WbnGWEQiwFHAfd6s/rC/viCb+6grIvJToBW405u1AqhU1T2AS4C7RKQ4gyH1y98uzclsesCR8f3VSfmwgd9/Y0FJBMuBirTpcm9exohILu5HvlNVHwBQ1c9VtU1VU8BNbGzOyFi8qrrce10F/MuL4fP2Jh/vdVWm4/JMB95S1c+9GLO+v9L0dh9lLEYR+SZwBHCqV4DgNb2s8cbfxLW/j/NiSG8+8iWuLfjtMrm/coBjgRlp8WZ0f3VWPpDBv7GgJIJZwFgRGeUdZZ4EzMzUxr32x5uBBar6h7T56e3rxwDtVzPMBE4SkTwRGQWMxZ2g6uu4CkSkqH0cd6Jxnrf99isOzgQeSovrDO+qhX2B2rSqqx82OUrL9v7qoLf76HHgUBEZ6DWLHOrN61MiMg34EXCUqjalzS8TkbA3Phq3jz72YqsTkX29v9Mz0r5LX8bV298uk/+zU4H3VXVDk08m91dX5QOZ/BvbmrPd29KAO9P+AS6z/zTD2z4AV62bC8zxhsOA24F3vfkzgeFpn/mpF+tCtvKqhG7iGo27GuMdYH77fgEGA08DHwJPAYO8+QJc58X1LjDZx31WAKwBStLmZWV/4ZLRCiCJa3c9Z0v2Ea7NfpE3nOVTXItw7cTtf2d/85Y9zvuN5wBvAUemrWcyrmD+CPgLXo8DfRxXr3+7vv6f7Swub/4twIUdls3k/uqqfMjY35h1MWGMMQEXlKYhY4wxXbBEYIwxAWeJwBhjAs4SgTHGBJwlAmOMCThLBCawRKTBe60WkVP6eN0/6TD9Sl+u35i+ZInAGNfTZK8SgXc3anc2SQSqul8vYzImYywRGAO/Bg4U1+/8D0QkLK5f/1leJ2kXAIjIQSLyoojMBN7z5j3oddg3v73TPhH5NRDz1nenN6+99iHeuueJ69P+xLR1Pyci/xT3PIE7vTtOjfHd5o5qjAmCy3B95R8B4BXotaq6l4jkAS+LyBPespOACeq6TAY4W1XXikgMmCUi96vqZSLyHVXdvZNtHYvreG0iUOp95gXvvT2AXYDPgJeB/YGX+v7rGrMpqxEY80WH4vpymYPrDngwrq8ZgDfSkgDA90TkHVzf/xVpy3XlAOBudR2wfQ48D+yVtu5l6jpmm4NrsjLGd1YjMOaLBPiuqm7SYZeIHAQ0dpieCkxR1SYReQ6IbsV2E2njbdj/p8kQqxEYA/W4RwS2exz4ltc1MCIyzuudtaMSYJ2XBHbCPTawXbL98x28CJzonYcowz0+0e+eUo3plh1xGON6fWzzmnhuAf6Ea5Z5yzthu5rOH0f4GHChiCzA9Zz5Wtp7NwJzReQtVT01bf6/gCm4Hl8V+JGqrvQSiTFZYb2PGmNMwFnTkDHGBJwlAmOMCThLBMYYE3CWCIwxJuAsERhjTMBZIjDGmICzRGCMMQH3/wE/a/our4mJPQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k6Km60WJY9K1"
      }
    }
  ]
}